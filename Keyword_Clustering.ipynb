{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5avAoo4fO/NZfi5oLTN9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/razvanantonescu/seo-keyword-clustering/blob/main/Keyword_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9ibgQDubC3F",
        "outputId": "30e0ef60-a6b2-4b2a-a219-e7c41af50828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyword clustering with topic names complete! Check 'clusters_with_topics.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 300 reached. Increase it to improve convergence.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Read keywords from text file\n",
        "with open(\"keywords.txt\", \"r\") as f:\n",
        "    keywords = f.read().splitlines()\n",
        "\n",
        "# Create a Tf-idf representation of the keywords\n",
        "vectorizer = TfidfVectorizer(stop_words='english') # Adding stop words can improve topic quality\n",
        "X = vectorizer.fit_transform(keywords)\n",
        "\n",
        "# Perform Affinity Propagation clustering\n",
        "af = AffinityPropagation().fit(X)\n",
        "cluster_centers_indices = af.cluster_centers_indices_\n",
        "labels = af.labels_\n",
        "\n",
        "# Get the number of clusters found\n",
        "n_clusters = len(cluster_centers_indices)\n",
        "\n",
        "# Group keywords by cluster\n",
        "clustered_keywords = [[] for _ in range(n_clusters)]\n",
        "for i, label in enumerate(labels):\n",
        "    clustered_keywords[label].append(keywords[i])\n",
        "\n",
        "# Apply NMF to each cluster to get topic names\n",
        "n_top_words = 3  # You can adjust this to get more or fewer words for the topic name\n",
        "topic_names = []\n",
        "for i in range(n_clusters):\n",
        "      if clustered_keywords[i]:\n",
        "        cluster_text = [\" \".join(clustered_keywords[i])] # Combine keywords in a cluster into a single \"document\"\n",
        "        cluster_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        cluster_tfidf = cluster_vectorizer.fit_transform(cluster_text)\n",
        "        if cluster_tfidf.shape[1] > 0: # Ensure there are words to model\n",
        "            nmf = NMF(n_components=1, random_state=42, max_iter=300, alpha_W=0.00001, l1_ratio=0.5).fit(cluster_tfidf) # n_components=1 to get one topic per cluster\n",
        "            feature_names = cluster_vectorizer.get_feature_names_out()\n",
        "            top_words_indices = nmf.components_[0].argsort()[:-n_top_words - 1:-1]\n",
        "            topic_words = [feature_names[i] for i in top_words_indices]\n",
        "            topic_names.append(\" \".join(topic_words))\n",
        "        else:\n",
        "            topic_names.append(f\"Cluster {i+1} (Empty)\") # Handle empty clusters\n",
        "      else:\n",
        "        topic_names.append(f\"Cluster {i+1} (No keywords)\") # Handle clusters with no keywords\n",
        "\n",
        "\n",
        "# Write the clusters to a csv file with meaningful names\n",
        "with open(\"clusters_with_topics.csv\", \"w\", newline=\"\") as f: # Keep the file open throughout writing\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Cluster Name\", \"Keyword\"])\n",
        "    for i in range ( n_clusters ) :\n",
        "        cluster_keywords = [ keywords [ j ] for j in range ( len ( labels ) ) if labels [ j ] == i ]\n",
        "        if cluster_keywords :\n",
        "            for keyword in cluster_keywords :\n",
        "                writer.writerow ( [ topic_names [ i ] , keyword ] )\n",
        "        else :\n",
        "            writer.writerow ( [ topic_names [ i ] , \"No keywords in this cluster\" ] )\n",
        "print(\"Keyword clustering with topic names complete! Check 'clusters_with_topics.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "-XmWBjCjdLE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}